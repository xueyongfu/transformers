
;无数据清洗,长度400
[bert]
data_dir = Data/train_data
model_type = bert
;model_name_or_path = os.path.expanduser("~") + "/models/chinese/bert/pytorch/bert-base-chinese"
model_name_or_path = output_无简单数据清洗/checkpoint-148000
task_name = sst-2
log_name = 无简单清洗再训练
output_dir = output
max_seq_length = 400
do_train = True
do_eval = True
eval_all_checkpoints = False
evaluate_during_training = True
do_lower_case = True
per_gpu_train_batch_size = 4
per_gpu_eval_batch_size = 4
gradient_accumulation_steps = 4
learning_rate = 5e-5
num_train_epochs = 20
warmup_steps = 500
overwrite_output_dir = False
overwrite_cache = False
logging_steps = 1000
save_steps = 1000


;简单的数据清洗,限制句子长度在靠右200
;[bert]
;data_dir = Data/train_data
;model_type = bert
;model_name_or_path = os.path.expanduser("~") + "/models/chinese/bert/pytorch/bert-base-chinese"
;task_name = sst-2
;log_name = ''
;output_dir = output_简单数据清洗+长度200/checkpoint-148000
;max_seq_length = 200
;do_train = False
;do_eval = True
;eval_all_checkpoints = False
;evaluate_during_training = True
;do_lower_case = True
;per_gpu_train_batch_size = 4
;per_gpu_eval_batch_size = 4
;gradient_accumulation_steps = 4
;learning_rate = 5e-5
;num_train_epochs = 20
;warmup_steps = 500
;overwrite_output_dir = False
;overwrite_cache = True
;logging_steps = 1000
;save_steps = 1000

